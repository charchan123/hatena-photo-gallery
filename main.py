import os
import glob
import json
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
import html
import piexif
from openai import OpenAI
import time

# ======================================================
# ğŸ§  OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå®‰å®šç‰ˆ / timeout=20ç§’ï¼‰
# ======================================================
# OPENAI_API_KEY ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•å–å¾—ï¼ˆGitHub Actions ã‚‚OKï¼‰
client = OpenAI(timeout=20)

# ===========================
# EXIF æ–‡å­—ã‚¯ãƒªãƒ¼ãƒ³é–¢æ•°
# ===========================
def clean_exif_str(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\x00", "")
    s = re.sub(r"[ï¿½]+", "", s)
    return s.strip()


# ====== è¨­å®š ======
HATENA_USER = os.getenv("HATENA_USER")
HATENA_BLOG_ID = os.getenv("HATENA_BLOG_ID")
HATENA_API_KEY = os.getenv("HATENA_API_KEY")

if not all([HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY]):
    raise EnvironmentError(
        "ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™ã€‚HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )

ARTICLES_DIR = "articles"
OUTPUT_DIR = "output"

# ====== EXIF ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ======
CACHE_DIR = "cache"
CACHE_FILE = os.path.join(CACHE_DIR, "exif-cache.json")

# ====== èª¬æ˜æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ======
DESC_CACHE_FILE = os.path.join(CACHE_DIR, "description-cache.json")

# ====== ã¯ã¦ãª API ======
ATOM_ENDPOINT = f"https://blog.hatena.ne.jp/{HATENA_USER}/{HATENA_BLOG_ID}/atom/entry"
AUTH = (HATENA_USER, HATENA_API_KEY)
HEADERS = {}

AIUO_GROUPS = {
    "ã‚è¡Œ": list("ã‚ã„ã†ãˆãŠã‚¢ã‚¤ã‚¦ã‚¨ã‚ª"),
    "ã‹è¡Œ": list("ã‹ããã‘ã“ã‚«ã‚­ã‚¯ã‚±ã‚³ãŒããã’ã”ã‚¬ã‚®ã‚°ã‚²ã‚´"),
    "ã•è¡Œ": list("ã•ã—ã™ã›ãã‚µã‚·ã‚¹ã‚»ã‚½ã–ã˜ãšãœãã‚¶ã‚¸ã‚ºã‚¼ã‚¾"),
    "ãŸè¡Œ": list("ãŸã¡ã¤ã¦ã¨ã‚¿ãƒãƒ„ãƒ†ãƒˆã ã¢ã¥ã§ã©ãƒ€ãƒ‚ãƒ…ãƒ‡ãƒ‰"),
    "ãªè¡Œ": list("ãªã«ã¬ã­ã®ãƒŠãƒ‹ãƒŒãƒãƒ"),
    "ã¯è¡Œ": list("ã¯ã²ãµã¸ã»ãƒãƒ’ãƒ•ãƒ˜ãƒ›ã°ã³ã¶ã¹ã¼ãƒãƒ“ãƒ–ãƒ™ãƒœã±ã´ã·ãºã½ãƒ‘ãƒ”ãƒ—ãƒšãƒ"),
    "ã¾è¡Œ": list("ã¾ã¿ã‚€ã‚ã‚‚ãƒãƒŸãƒ ãƒ¡ãƒ¢"),
    "ã‚„è¡Œ": list("ã‚„ã‚†ã‚ˆãƒ¤ãƒ¦ãƒ¨"),
    "ã‚‰è¡Œ": list("ã‚‰ã‚Šã‚‹ã‚Œã‚ãƒ©ãƒªãƒ«ãƒ¬ãƒ­"),
    "ã‚è¡Œ": list("ã‚ã‚’ã‚“ãƒ¯ãƒ²ãƒ³"),
}

# ------------------------------------------------------
# CSS & JSï¼ˆãã®ã¾ã¾ï¼ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
# ------------------------------------------------------
STYLE_TAG = """<style>
html, body {
  margin: 0;
  padding: 0;
  overflow-y: hidden;
}
body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  background:#fafafa;
  color:#333;
  padding:16px;
  box-sizing:border-box;
}
...
</style>"""

LIGHTGALLERY_TAGS = """
<link rel="stylesheet" href="./lightgallery/lightgallery-bundle.min.css">
<link rel="stylesheet" href="./lightgallery/lg-thumbnail.css">
<script src="./lightgallery/lightgallery.min.js"></script>
<script src="./lightgallery/lg-zoom.min.js"></script>
<script src="./lightgallery/lg-thumbnail.min.js"></script>
"""

SCRIPT_TAG = """<script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
<script>
...
</script>
"""

# ===========================
# EXIF ã‚­ãƒ£ãƒƒã‚·ãƒ¥ load/save
# ===========================
def load_exif_cache():
    if not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, dict) else {}
    except:
        return {}

def save_exif_cache(cache: dict):
    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


# ===========================
# EXIF æŠ½å‡º
# ===========================
def extract_exif_from_bytes(jpeg_bytes: bytes):
    try:
        exif_dict = piexif.load(jpeg_bytes)
    except:
        return {}

    try:
        zero = exif_dict.get("0th", {})
        exif = exif_dict.get("Exif", {})

        # ---- Camera Model ----
        model = zero.get(piexif.ImageIFD.Model, b"")
        model = clean_exif_str(model.decode(errors="ignore")) if isinstance(model, bytes) else ""

        # ---- Lens ----
        lens = exif.get(piexif.ExifIFD.LensModel, b"")
        lens = clean_exif_str(lens.decode(errors="ignore")) if isinstance(lens, bytes) else ""

        if "IS" in lens:
            lens = lens.split("IS")[0] + "IS"

        # ---- ISO ----
        iso = exif.get(piexif.ExifIFD.ISOSpeedRatings)
        if isinstance(iso, (list, tuple)):
            iso = iso[0]
        iso_str = str(iso) if iso else ""

        # ---- Få€¤ ----
        fnum = exif.get(piexif.ExifIFD.FNumber)
        f_str = ""
        if fnum and isinstance(fnum, tuple) and fnum[1]:
            f_str = f"f/{(fnum[0] / fnum[1]):.1f}"

        # ---- éœ²å‡º ----
        exposure = exif.get(piexif.ExifIFD.ExposureTime)
        exposure_str = ""
        if exposure and isinstance(exposure, tuple) and exposure[1]:
            exposure_str = f"{exposure[0]}/{exposure[1]}"

        # ---- ç„¦ç‚¹è·é›¢ ----
        focal = exif.get(piexif.ExifIFD.FocalLength)
        focal_str = ""
        if focal and isinstance(focal, tuple) and focal[1]:
            fv = focal[0] / focal[1]
            focal_str = f"{int(round(fv))}mm" if abs(fv - round(fv)) < 0.1 else f"{fv:.1f}mm"

        # ---- æ—¥ä»˜ ----
        dt = exif.get(piexif.ExifIFD.DateTimeOriginal, b"")
        dt = dt.decode(errors="ignore") if isinstance(dt, bytes) else ""
        date_str = dt.split(" ")[0].replace(":", "/") if dt else ""

        return {
            "model": model,
            "lens": lens,
            "iso": iso_str,
            "f": f_str,
            "exposure": exposure_str,
            "focal": focal_str,
            "date": date_str,
        }

    except:
        return {}

# ===========================
# EXIF ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰
# ===========================
def build_exif_cache(entries, cache: dict):
    os.makedirs(CACHE_DIR, exist_ok=True)

    all_srcs = sorted({e["src"] for e in entries})

    for src in all_srcs:
        if src in cache:
            continue

        print(f"ğŸ” EXIFå–å¾—: {src}")

        try:
            r = requests.get(src, timeout=3)
            if r.status_code != 200:
                print(f"  â†ª HTTP {r.status_code} â†’ ç©ºãƒ‡ãƒ¼ã‚¿")
                cache[src] = {}
                continue

            try:
                exif_data = extract_exif_from_bytes(r.content) or {}
                print(f"  â†ª EXIFå–å¾—OK: {exif_data}")
                cache[src] = exif_data
            except Exception as e:
                print(f"  â†ª EXIFã‚¨ãƒ©ãƒ¼: {e} â†’ ç©ºãƒ‡ãƒ¼ã‚¿")
                cache[src] = {}

        except Exception as e:
            print(f"  â†ª å–å¾—å¤±æ•—: {e} â†’ ç©ºãƒ‡ãƒ¼ã‚¿")
            cache[src] = {}

    return cache


# ===========================
# â­ èª¬æ˜æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ load/save
# ===========================
def load_description_cache():
    if not os.path.exists(DESC_CACHE_FILE):
        return {}
    try:
        with open(DESC_CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, dict) else {}
    except:
        return {}

def save_description_cache(cache: dict):
    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(DESC_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


# ===========================
# â­ GPT èª¬æ˜æ–‡ç”Ÿæˆï¼ˆå®‰å®šç‰ˆï¼‰
# ===========================
def generate_description_via_gpt(name: str) -> str:
    max_retry = 3
    timeout_sec = 15

    prompt = f"""
ã‚ãªãŸã¯ã€Œã‚­ãƒã‚³å°‚é–€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å›³é‘‘ã®ç·¨é›†ãƒ©ã‚¤ã‚¿ãƒ¼ã€ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚­ãƒã‚³åã«ã¤ã„ã¦ã€ä¸€èˆ¬å‘ã‘å›³é‘‘ã¨ã—ã¦å®‰å…¨ã§èª­ã¿ã‚„ã™ã„èª¬æ˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒã‚³å: {name}

ã€æ¡ä»¶ã€‘
ãƒ»å¤–è¦‹ã‹ã‚‰æ–­å®šã—ãªã„è¡¨ç¾ï¼ˆã€œã“ã¨ãŒå¤šã„ç­‰ï¼‰
ãƒ»ç™ºç”Ÿç’°å¢ƒãƒ»å­£ç¯€ãƒ»è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆãƒ»ä¼¼ãŸç¨®ã®æ³¨æ„
ãƒ»é£Ÿæ¯’ã¯çµ¶å¯¾æ–­è¨€ã—ãªã„ã€‚å®‰å…¨æ³¨æ„ã‚’å…¥ã‚Œã‚‹
ãƒ»300ã€œ500æ–‡å­—
ãƒ»2ã€œ3æ®µè½ã«åˆ†ã‘ã‚‹
""".strip()

    def call_openai():
        # æœ€æ–°APIï¼ˆResponsesï¼‰ã‚’ä½¿ç”¨
        return client.responses.create(
            model="gpt-4o-mini",
            input=prompt,
        )

    # â˜… Responses API ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ã«æŠ½å‡ºã™ã‚‹é–¢æ•°
    def extract_text(res):
        try:
            return res.output[0].content[0].text
        except:
            try:
                return res.output_text
            except:
                return ""

    for attempt in range(1, max_retry + 1):
        try:
            print(f"ğŸ§  èª¬æ˜æ–‡ç”Ÿæˆä¸­ï¼ˆè©¦è¡Œ {attempt}/{max_retry}ï¼‰: {name}")

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¬ãƒ¼ãƒ‰
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(call_openai)
                res = future.result(timeout=timeout_sec)

            text = extract_text(res).strip()
            if text:
                return text

        except FutureTimeout:
            print(f"âš ï¸ GPTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {name} â†’ å†è©¦è¡Œ")
        except Exception as e:
            print(f"âš ï¸ GPTã‚¨ãƒ©ãƒ¼: {name}: {e}")

        time.sleep(1)

    print(f"âš ï¸ GPTå¤±æ•—: {name} â†’ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€è¿”å´")
    return f"{name} ã®èª¬æ˜æ–‡ã¯æº–å‚™ä¸­ã§ã™ã€‚"

# ===========================
# â­ èª¬æ˜æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ â†’ GPTï¼ˆå®‰å®šå‡¦ç†ï¼‰
# ===========================
def get_ai_description(name: str, desc_cache: dict) -> str:
    """
    ãƒ»ã™ã§ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°å³è¿”ã™ï¼ˆAPIç„¡æ–™ï¼‰
    ãƒ»ãªã‘ã‚Œã° GPT ç”Ÿæˆ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    """
    key = name.strip()
    if key in desc_cache:
        return desc_cache[key]

    text = generate_description_via_gpt(key)
    desc_cache[key] = text
    save_description_cache(desc_cache)
    return text

# ===========================
# ã¯ã¦ãªãƒ–ãƒ­ã‚° API â†’ å…¨è¨˜äº‹å–å¾—
# ===========================
def fetch_hatena_articles_api():
    os.makedirs(ARTICLES_DIR, exist_ok=True)
    print("ğŸ“¡ ã¯ã¦ãªãƒ–ãƒ­ã‚°APIå–å¾—ä¸­â€¦")
    url = ATOM_ENDPOINT
    count = 0

    while url:
        print(f"ğŸ”— Fetch: {url}")
        r = requests.get(url, auth=AUTH, headers=HEADERS)
        if r.status_code != 200:
            raise RuntimeError(f"âŒ APIå¤±æ•—: {r.status_code} {r.text}")

        root = ET.fromstring(r.text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}
        entries = root.findall("atom:entry", ns)

        for i, entry in enumerate(entries, 1):
            content = entry.find("atom:content", ns)
            if content is None:
                continue
            html_content = content.text or ""
            filename = f"{ARTICLES_DIR}/article_{count+i}.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"  â†’ ä¿å­˜: {filename}")

        count += len(entries)
        next_link = root.find("atom:link[@rel='next']", ns)
        url = next_link.attrib["href"] if next_link is not None else None

    print(f"ğŸ“¦ åˆè¨ˆ {count} ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜å®Œäº†")


# ===========================
# HTML â†’ ç”»åƒ + alt æŠ½å‡º
# ===========================
def fetch_images():
    print("ğŸ“‚ HTMLâ†’ç”»åƒæŠ½å‡ºä¸­â€¦")
    entries = []

    exclude_patterns = [
        r'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
        r'^\d{4}å¹´',
        r'^ã“ã®è¨˜äº‹ã‚’ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ $',
        r'^ãƒ¯è¡Œ$',
        r'ã‚­ãƒã‚³ã¨ç”°èˆéŠã³',
    ]

    for html_file in glob.glob(f"{ARTICLES_DIR}/*.html"):
        with open(html_file, encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")

        body_div = soup.find(class_="entry-body") or soup

        for iframe in body_div.find_all("iframe"):
            if any(re.search(p, iframe.get("title", "")) for p in exclude_patterns):
                iframe.decompose()

        for a in body_div.find_all("a"):
            if any(re.search(p, a.get_text(strip=True)) for p in exclude_patterns):
                a.decompose()

        for img in body_div.find_all("img"):
            alt = (img.get("alt") or "").strip()
            src = img.get("src")
            if not alt or not src:
                continue
            if any(re.search(p, alt) for p in exclude_patterns):
                continue

            entries.append({"alt": alt, "src": src})

    print(f"ğŸ§© æŠ½å‡ºç”»åƒæ•°: {len(entries)}")
    return entries


# ===========================
# äº”åéŸ³åˆ†é¡
# ===========================
def get_aiuo_group(name):
    if not name:
        return "ãã®ä»–"
    first = name[0]
    for g, chars in AIUO_GROUPS.items():
        if first in chars:
            return g
    return "ãã®ä»–"


# ===========================
# EXIF â†’ caption HTML
# ===========================
def build_caption_html(alt, exif):
    title = html.escape(alt)

    parts = []
    if exif.get("model"):
        parts.append(f"ã‚«ãƒ¡ãƒ©ï¼š{html.escape(exif['model'])}")
    if exif.get("lens"):
        parts.append(f"ãƒ¬ãƒ³ã‚ºï¼š{html.escape(exif['lens'])}")
    if exif.get("iso"):
        parts.append(f"ISOï¼š{html.escape(exif['iso'])}")
    if exif.get("f"):
        parts.append(f"çµã‚Šï¼š{html.escape(exif['f'])}")
    if exif.get("exposure"):
        parts.append(f"ã‚·ãƒ£ãƒƒã‚¿ãƒ¼é€Ÿåº¦ï¼š{html.escape(exif['exposure'])}")
    if exif.get("focal"):
        parts.append(f"ç„¦ç‚¹è·é›¢ï¼š{html.escape(exif['focal'])}")
    if exif.get("date"):
        parts.append(f"æ’®å½±æ—¥ï¼š{html.escape(exif['date'])}")

    exif_line = " | ".join(parts)

    block = (
        "<div style='text-align:center;'>"
        f"<div style='font-weight:bold; font-size:1.2em; margin-bottom:6px;'>{title}</div>"
        f"<div style='font-size:0.9em;'>{exif_line}</div>"
        "</div>"
    )

    return html.escape(block, quote=True)


# ===========================
# ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç”Ÿæˆ
# ===========================
def generate_gallery(entries, exif_cache, desc_cache):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    grouped = {}
    for e in entries:
        grouped.setdefault(e["alt"], []).append(e["src"])

    group_links = " | ".join([f'<a href="{g}.html">{g}</a>' for g in AIUO_GROUPS.keys()])
    group_links_html = f"<div style='margin-top:40px; text-align:center;'>{group_links}</div>"

    def safe_filename(name):
        name = re.sub(r'[:<>"|*?/\\\r\n]', "_", name).strip()
        return name if name else "unnamed"

    # ---- å„ã‚­ãƒã‚³ãƒšãƒ¼ã‚¸ ----
    for alt, imgs in grouped.items():
        html_parts = []

        # â­ GPTèª¬æ˜æ–‡ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰
        ai_text = get_ai_description(alt, desc_cache)
        body_html = "".join(f"<p>{html.escape(p)}</p>" for p in ai_text.split("\n") if p.strip())

        card_html = (
            "<div class='info-card'>"
            f"<h3>{html.escape(alt)}</h3>"
            f"{body_html}"
            "</div>"
        )
        html_parts.append(card_html)

        # ç”»åƒã‚®ãƒ£ãƒ©ãƒªãƒ¼
        html_parts.append("<div class='gallery'>")
        for src in imgs:
            caption_attr = build_caption_html(alt, exif_cache.get(src, {}))
            thumb = src + "?width=300"

            html_parts.append(
                f'<a class="gallery-item" href="{src}" '
                f'data-exthumbimage="{thumb}" '
                f'data-sub-html="{caption_attr}">'
                f'<img src="{src}" alt="{html.escape(alt)}" loading="lazy">'
                "</a>"
            )
        html_parts.append("</div>")

        html_parts.append("""
        <div style='margin-top:40px;text-align:center;'>
            <a href='javascript:history.back()' style='color:#007acc;'>â† æˆ»ã‚‹</a>
        </div>
        """)

        html_parts.append(STYLE_TAG)
        html_parts.append(LIGHTGALLERY_TAGS)
        html_parts.append(SCRIPT_TAG)

        page_html = "".join(html_parts)
        safe = safe_filename(alt)
        with open(f"{OUTPUT_DIR}/{safe}.html", "w", encoding="utf-8") as f:
            f.write(page_html)

    # ---- äº”åéŸ³ãƒšãƒ¼ã‚¸ ----
    aiuo_dict = {k: [] for k in AIUO_GROUPS}
    for alt in grouped:
        g = get_aiuo_group(alt)
        aiuo_dict[g].append(alt)

    for g, names in aiuo_dict.items():
        html_parts = [f"<h2>{g}ã®ã‚­ãƒã‚³</h2><ul>"]
        for n in sorted(names):
            html_parts.append(f'<li><a href="{safe_filename(n)}.html">{html.escape(n)}</a></li>')
        html_parts.append("</ul>")
        html_parts.append(group_links_html)
        html_parts.append(STYLE_TAG)
        html_parts.append(LIGHTGALLERY_TAGS)
        html_parts.append(SCRIPT_TAG)

        with open(f"{OUTPUT_DIR}/{safe_filename(g)}.html", "w", encoding="utf-8") as f:
            f.write("".join(html_parts))

    # ---- index ----
    index_parts = ["<h2>äº”åéŸ³åˆ†é¡</h2><ul>"]
    for g in AIUO_GROUPS:
        index_parts.append(f'<li><a href="{safe_filename(g)}.html">{g}</a></li>')
    index_parts.append("</ul>")
    index_parts.append(STYLE_TAG)
    index_parts.append(LIGHTGALLERY_TAGS)
    index_parts.append(SCRIPT_TAG)

    with open(f"{OUTPUT_DIR}/index.html", "w", encoding="utf-8") as f:
        f.write("".join(index_parts))

    print("âœ… ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒšãƒ¼ã‚¸ç”Ÿæˆå®Œäº†")


# ===========================
# ãƒ¡ã‚¤ãƒ³
# ===========================
if __name__ == "__main__":
    fetch_hatena_articles_api()
    entries = fetch_images()
    if entries:
        exif_cache = load_exif_cache()
        exif_cache = build_exif_cache(entries, exif_cache)
        save_exif_cache(exif_cache)

        desc_cache = load_description_cache()

        generate_gallery(entries, exif_cache, desc_cache)
    else:
        print("âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
