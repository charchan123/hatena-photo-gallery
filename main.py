import os
import glob
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
from io import BytesIO
from PIL import Image, ExifTags

# ====== è¨­å®š ======
HATENA_USER = os.getenv("HATENA_USER")
HATENA_BLOG_ID = os.getenv("HATENA_BLOG_ID")
HATENA_API_KEY = os.getenv("HATENA_API_KEY")

if not all([HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY]):
    raise EnvironmentError(
        "ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™ã€‚HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )

ARTICLES_DIR = "articles"
OUTPUT_DIR = "output"

# ====== API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ======
ATOM_ENDPOINT = f"https://blog.hatena.ne.jp/{HATENA_USER}/{HATENA_BLOG_ID}/atom/entry"
AUTH = (HATENA_USER, HATENA_API_KEY)
HEADERS = {}

AIUO_GROUPS = {
    "ã‚è¡Œ": list("ã‚ã„ã†ãˆãŠã‚¢ã‚¤ã‚¦ã‚¨ã‚ª"),
    "ã‹è¡Œ": list("ã‹ããã‘ã“ã‚«ã‚­ã‚¯ã‚±ã‚³ãŒããã’ã”ã‚¬ã‚®ã‚°ã‚²ã‚´"),
    "ã•è¡Œ": list("ã•ã—ã™ã›ãã‚µã‚·ã‚¹ã‚»ã‚½ã–ã˜ãšãœãã‚¶ã‚¸ã‚ºã‚¼ã‚¾"),
    "ãŸè¡Œ": list("ãŸã¡ã¤ã¦ã¨ã‚¿ãƒãƒ„ãƒ†ãƒˆã ã¢ã¥ã§ã©ãƒ€ãƒ‚ãƒ…ãƒ‡ãƒ‰"),
    "ãªè¡Œ": list("ãªã«ã¬ã­ã®ãƒŠãƒ‹ãƒŒãƒãƒ"),
    "ã¯è¡Œ": list("ã¯ã²ãµã¸ã»ãƒãƒ’ãƒ•ãƒ˜ãƒ›ã°ã³ã¶ã¹ã¼ãƒãƒ“ãƒ–ãƒ™ãƒœã±ã´ã·ãºã½ãƒ‘ãƒ”ãƒ—ãƒšãƒ"),
    "ã¾è¡Œ": list("ã¾ã¿ã‚€ã‚ã‚‚ãƒãƒŸãƒ ãƒ¡ãƒ¢"),
    "ã‚„è¡Œ": list("ã‚„ã‚†ã‚ˆãƒ¤ãƒ¦ãƒ¨"),
    "ã‚‰è¡Œ": list("ã‚‰ã‚Šã‚‹ã‚Œã‚ãƒ©ãƒªãƒ«ãƒ¬ãƒ­"),
    "ã‚è¡Œ": list("ã‚ã‚’ã‚“ãƒ¯ãƒ²ãƒ³"),
}

# ====== å…±é€šã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆMasonryï¼‹EXIFãƒœãƒƒã‚¯ã‚¹ï¼‰ ======
STYLE_TAG = """<style>
html, body {
  margin: 0;
  padding: 0;
  overflow-y: hidden;
}
body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  background:#fafafa;
  color:#333;
  padding:16px;
  box-sizing:border-box;
}

.gallery {
  column-count: 2;
  column-gap: 10px;
  max-width: 900px;
  margin: 0 auto;
  visibility: hidden;
}
.gallery a.gallery-item{
  display: block;
  break-inside: avoid;
  margin-bottom: 10px;
  border-radius: 8px;
  overflow: hidden;
}
.gallery img {
  width: 100%;
  height: auto;
  display: block;
  cursor: zoom-in;
  transition: opacity 0.6s ease, transform 0.6s ease;
  opacity: 0;
  transform: translateY(10px);
}
.gallery img.visible {
  opacity: 1;
  transform: translateY(0);
}

@media (max-width: 480px) {
  .gallery { column-count: 1; }
}

/* LightGallery ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼†EXIFç”¨ */
.lg-caption-title {
  font-size: 16px;
  font-weight: 700;
  margin-bottom: 4px;
}
.exif-box {
  margin-top: 6px;
  padding: 6px 8px;
  border-radius: 6px;
  background: rgba(0,0,0,0.55);
  font-size: 13px;
  line-height: 1.6;
}
.exif-box strong {
  font-size: 13px;
}
</style>"""

# ====== LightGallery èª­ã¿è¾¼ã¿ã‚¿ã‚°ï¼ˆbundleç‰ˆ v2ï¼‰ ======
LIGHTGALLERY_TAGS = """
<!-- LightGallery CSS -->
<link rel="stylesheet" href="./lightgallery/lightgallery-bundle.min.css">

<!-- LightGallery JS (bundleç‰ˆ â†’ v2) -->
<script src="./lightgallery/lightgallery-bundle.min.js"></script>
"""

# ====== ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ï¼‹è¦ªã¨ã®é€£æºã®ã¿ï¼‰ ======
SCRIPT_TAG = """<script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded", () => {
  function sendHeight() {
    const height = document.documentElement.scrollHeight;
    window.parent.postMessage({ type:"setHeight", height }, "*");
  }

  const gallery = document.querySelector(".gallery");
  if (gallery) {

    // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
    const fadeObs = new IntersectionObserver(entries=>{
      entries.forEach(e=>{
        if(e.isIntersecting){
          e.target.classList.add("visible");
          fadeObs.unobserve(e.target);
        }
      });
    }, {threshold:0.1});
    gallery.querySelectorAll("img").forEach(img=>fadeObs.observe(img));

    imagesLoaded(gallery, () => {
      console.log("âœ… imagesLoaded å®Œäº†");
      gallery.style.visibility="visible";
      sendHeight();

      // ===== LightGallery v2 åˆæœŸåŒ– =====
      const lg = lightGallery(gallery, {
        selector: 'a.gallery-item',
        plugins: [lgZoom, lgThumbnail],
        speed: 400,
        download: false,
        zoom: true,
        thumbnail: true,
      });
      console.log("âœ… lg ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹:", lg);

      // â‘  ã‚µãƒ ãƒã‚¤ãƒ«ã‚¯ãƒªãƒƒã‚¯æ™‚ã«å¼·åˆ¶ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³
      gallery.querySelectorAll("a.gallery-item").forEach((a) => {
        a.addEventListener("click", () => {
          const el = document.documentElement;
          if (el.requestFullscreen) el.requestFullscreen();
          else if (el.webkitRequestFullscreen) el.webkitRequestFullscreen();
          else if (el.msRequestFullscreen) el.msRequestFullscreen();
        });
      });

      // â‘¡ ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚’é–‰ã˜ã‚‹å‰ã«ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³è§£é™¤ + è¦ªã«é€šçŸ¥
      gallery.addEventListener("lgBeforeClose", () => {
        console.log("ğŸ“¤ å­ï¼šLG before close ç™ºç«");
        if (document.fullscreenElement) {
          document.exitFullscreen().catch(()=>{});
        }
        window.parent.postMessage({ type: "lgClosed" }, "*");
      });

      // â‘¢ ESC ã§ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³è§£é™¤ã•ã‚ŒãŸã‚‰ã‚®ãƒ£ãƒ©ãƒªãƒ¼é–‰ã˜ã¦è¦ªã¸é€šçŸ¥
      document.addEventListener("fullscreenchange", () => {
        if (!document.fullscreenElement) {
          console.log("ğŸ“¤ å­ï¼šfullscreenchangeç™ºç« â†’ è¦ªã« lgClosed ã‚’é€ä¿¡");
          try {
            lg.closeGallery();
            window.parent.postMessage({ type: "lgClosed" }, "*");
          } catch(e) {}
        }
      });

      // â‘£ äº”åéŸ³ãƒªãƒ³ã‚¯/æˆ»ã‚‹ãƒªãƒ³ã‚¯ã®ã‚¯ãƒªãƒƒã‚¯ â†’ è¦ªã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¾é ¼
      document.addEventListener("click", (e) => {
        const a = e.target.closest("a");
        if (!a) return;
        const txt = a.textContent || "";

        if (/ã‚è¡Œ|ã‹è¡Œ|ã•è¡Œ|ãŸè¡Œ|ãªè¡Œ|ã¯è¡Œ|ã¾è¡Œ|ã‚„è¡Œ|ã‚‰è¡Œ|ã‚è¡Œ/.test(txt)) {
          window.parent.postMessage({ type: "scrollToIframeTop" }, "*");
          return;
        }
        if (a.href && a.href.endsWith(".html")) {
          window.parent.postMessage({ type: "scrollToIframeTop" }, "*");
          return;
        }
        if (/æˆ»ã‚‹/.test(txt)) {
          window.parent.postMessage({ type: "scrollToIframeTop" }, "*");
          return;
        }
      });

    }); // imagesLoaded end
  }

  sendHeight();
  window.addEventListener("load", ()=>{ sendHeight(); setTimeout(sendHeight,800); setTimeout(sendHeight,2000); });
  window.addEventListener("message", e=>{ if(e.data?.type==="requestHeight") sendHeight(); });
  window.addEventListener("resize", sendHeight);
  new MutationObserver(sendHeight).observe(document.body,{childList:true,subtree:true});
});
</script>
"""

# ==========================
#  EXIF æŠ½å‡ºï¼ˆPythonå´ï¼‰
# ==========================
def extract_exif_from_url(url: str) -> dict:
    """ç”»åƒURLã‹ã‚‰ EXIF ã‚’èª­ã¿ã€è¡¨ç¤ºç”¨æ–‡å­—åˆ—ã‚’è¿”ã™"""
    try:
        print(f"ğŸ” EXIFå–å¾—: {url}")
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()

        img = Image.open(BytesIO(resp.content))
        exif_raw = img._getexif()
        if not exif_raw:
            print("  â†ª EXIFãªã—")
            return {}

        exif = {}
        for tag, value in exif_raw.items():
            tag_name = ExifTags.TAGS.get(tag, tag)
            exif[tag_name] = value

        # ã‚«ãƒ¡ãƒ©
        model = exif.get("Model", "")

        # ãƒ¬ãƒ³ã‚º
        lens = exif.get("LensModel", "") or exif.get("LensMake", "")

        # ISO
        iso = exif.get("ISOSpeedRatings") or exif.get("PhotographicSensitivity")
        if isinstance(iso, (list, tuple)):
            iso = iso[0] if iso else None

        # Få€¤
        fnumber = exif.get("FNumber")
        if isinstance(fnumber, tuple) and len(fnumber) == 2 and fnumber[1] != 0:
            f_val = fnumber[0] / fnumber[1]
            f_str = f"f/{f_val:.1f}"
        else:
            f_str = ""

        # ã‚·ãƒ£ãƒƒã‚¿ãƒ¼ã‚¹ãƒ”ãƒ¼ãƒ‰
        exposure = exif.get("ExposureTime")
        if isinstance(exposure, tuple) and len(exposure) == 2 and exposure[1] != 0:
            # 1/200 ã¿ãŸã„ãªè¡¨ç¤º
            if exposure[0] == 1:
                exposure_str = f"1/{exposure[1]}"
            else:
                exposure_str = f"{exposure[0]}/{exposure[1]}"
        else:
            exposure_str = str(exposure) if exposure else ""

        # ç„¦ç‚¹è·é›¢
        focal = exif.get("FocalLength")
        if isinstance(focal, tuple) and len(focal) == 2 and focal[1] != 0:
            focal_val = focal[0] / focal[1]
            focal_str = f"{focal_val:.0f}mm"
        else:
            focal_str = ""

        # æ’®å½±æ—¥ï¼ˆYYYY/MM/DDï¼‰
        dt = exif.get("DateTimeOriginal") or exif.get("DateTime")
        date_str = ""
        if isinstance(dt, str) and len(dt) >= 10:
            # å½¢å¼: 'YYYY:MM:DD HH:MM:SS'
            parts = dt.split(" ")[0].split(":")
            if len(parts) == 3:
                y, m, d = parts
                date_str = f"{y}/{m}/{d}"

        result = {
            "model": model or "",
            "lens": lens or "",
            "iso": str(iso) if iso else "",
            "f": f_str,
            "exposure": exposure_str,
            "focal": focal_str,
            "date": date_str,
        }
        print(f"  â†ª EXIFå–å¾—OK: {result}")
        return result

    except Exception as e:
        print(f"âš ï¸ EXIFå–å¾—å¤±æ•—: {url} ({e})")
        return {}

# ==========================
#  APIã‹ã‚‰å…¨è¨˜äº‹ã‚’å–å¾—
# ==========================
def fetch_hatena_articles_api():
    os.makedirs(ARTICLES_DIR, exist_ok=True)
    print("ğŸ“¡ ã¯ã¦ãªãƒ–ãƒ­ã‚°APIã‹ã‚‰å…¨è¨˜äº‹å–å¾—ä¸­â€¦")
    url = ATOM_ENDPOINT
    count = 0
    while url:
        print(f"ğŸ”— Fetching: {url}")
        r = requests.get(url, auth=AUTH, headers=HEADERS)
        if r.status_code != 200:
            raise RuntimeError(f"âŒ APIå–å¾—å¤±æ•—: {r.status_code} {r.text}")

        root = ET.fromstring(r.text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}
        entries = root.findall("atom:entry", ns)

        for i, entry in enumerate(entries, 1):
            content = entry.find("atom:content", ns)
            if content is None:
                continue
            html_content = content.text or ""
            filename = f"{ARTICLES_DIR}/article_{count+i}.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"âœ… ä¿å­˜å®Œäº†: {filename}")

        count += len(entries)
        next_link = root.find("atom:link[@rel='next']", ns)
        url = next_link.attrib["href"] if next_link is not None else None

    print(f"ğŸ“¦ åˆè¨ˆ {count} ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ==========================
#  HTMLã‹ã‚‰ç”»åƒã¨alt+EXIFã‚’æŠ½å‡º
# ==========================
def fetch_images():
    print("ğŸ“‚ HTMLã‹ã‚‰ç”»åƒæŠ½å‡ºä¸­â€¦")
    entries = []

    exclude_patterns = [
        r'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
        r'^\d{4}å¹´',
        r'^ã“ã®è¨˜äº‹ã‚’ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ $',
        r'^ãƒ¯è¡Œ$',
        r'ã‚­ãƒã‚³ã¨ç”°èˆéŠã³',
    ]

    for html_file in glob.glob(f"{ARTICLES_DIR}/*.html"):
        with open(html_file, encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")

        body_div = soup.find(class_="entry-body") or soup

        for iframe in body_div.find_all("iframe"):
            title = iframe.get("title", "")
            if any(re.search(p, title) for p in exclude_patterns):
                iframe.decompose()

        for a in body_div.find_all("a"):
            text = a.get_text(strip=True)
            if any(re.search(p, text) for p in exclude_patterns):
                a.decompose()

        imgs = body_div.find_all("img")
        for img in imgs:
            alt = (img.get("alt") or "").strip()
            src = img.get("src")
            if not alt or not src:
                continue
            if any(re.search(p, alt) for p in exclude_patterns):
                continue

            exif = extract_exif_from_url(src)

            entries.append({
                "alt": alt,
                "src": src,
                "exif": exif,
            })

    print(f"ğŸ§© ç”»åƒæ¤œå‡ºæ•°: {len(entries)} æš")
    return entries

# ==========================
#  äº”åéŸ³åˆ†é¡
# ==========================
def get_aiuo_group(name):
    if not name:
        return "ãã®ä»–"
    first = name[0]
    for group, chars in AIUO_GROUPS.items():
        if first in chars:
            return group
    return "ãã®ä»–"

# ==========================
#  data-sub-html ç”¨ HTMLç”Ÿæˆ
# ==========================
def build_sub_html(alt: str, exif: dict) -> str:
    parts = []
    parts.append("<div class='lg-caption'>")
    parts.append(f"<div class='lg-caption-title'>{alt}</div>")

    if exif:
        parts.append("<div class='exif-box'><strong>æ’®å½±æƒ…å ±</strong><br>")
        if exif.get("model"):
            parts.append(f"ã‚«ãƒ¡ãƒ©ï¼š{exif['model']}<br>")
        if exif.get("lens"):
            parts.append(f"ãƒ¬ãƒ³ã‚ºï¼š{exif['lens']}<br>")
        if exif.get("iso"):
            parts.append(f"ISOï¼š{exif['iso']}<br>")
        if exif.get("f"):
            parts.append(f"çµã‚Šï¼š{exif['f']}<br>")
        if exif.get("exposure"):
            parts.append(f"ã‚·ãƒ£ãƒƒã‚¿ãƒ¼é€Ÿåº¦ï¼š{exif['exposure']}<br>")
        if exif.get("focal"):
            parts.append(f"ç„¦ç‚¹è·é›¢ï¼š{exif['focal']}<br>")
        if exif.get("date"):
            parts.append(f"æ’®å½±æ—¥ï¼š{exif['date']}<br>")
        parts.append("</div>")  # .exif-box

    parts.append("</div>")  # .lg-caption

    html = "".join(parts)
    # data-sub-html ç”¨ã«ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    return html.replace("'", "&#39;")

# ==========================
#  ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç”Ÿæˆ
# ==========================
def generate_gallery(entries):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # alt ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆåŒã˜ã‚­ãƒã‚³åã§è¤‡æ•°æšå¯¾å¿œï¼‰
    grouped = {}
    for e in entries:
        grouped.setdefault(e["alt"], []).append(e)

    group_links = " | ".join([f'<a href="{g}.html">{g}</a>' for g in AIUO_GROUPS.keys()])
    group_links_html = f"<div style='margin-top:40px; text-align:center;'>{group_links}</div>"

    def safe_filename(name):
        name = re.sub(r'[:<>"|*?\\\\/\\r\\n]', '_', name)
        name = name.strip()
        if not name:
            name = "unnamed"
        return name

    # ---- å„ã‚­ãƒã‚³ã®ãƒšãƒ¼ã‚¸ ----
    for alt, items in grouped.items():
        html = f"<h2>{alt}</h2><div class='gallery'>"
        for item in items:
            src = item["src"]
            exif = item.get("exif") or {}
            thumb = src + "?width=300"

            sub_html = build_sub_html(alt, exif)

            html += (
                f"<a class='gallery-item' href='{src}' "
                f"data-exthumbimage='{thumb}' "
                f"data-sub-html='{sub_html}'>"
                f"<img src='{src}' alt='{alt}' loading='lazy'>"
                f"</a>"
            )

        html += "</div>"
        html += """
        <div style='margin-top:40px; text-align:center;'>
            <a href='javascript:history.back()' style='text-decoration:none;color:#007acc;'>â† æˆ»ã‚‹</a>
        </div>
        """
        html += STYLE_TAG + LIGHTGALLERY_TAGS + SCRIPT_TAG

        safe = safe_filename(alt)
        with open(f"{OUTPUT_DIR}/{safe}.html", "w", encoding="utf-8") as f:
            f.write(html)

    # ---- äº”åéŸ³ã‚°ãƒ«ãƒ¼ãƒ—ãƒšãƒ¼ã‚¸ ----
    aiuo_dict = {k: [] for k in AIUO_GROUPS.keys()}
    for alt in grouped.keys():
        g = get_aiuo_group(alt)
        if g in aiuo_dict:
            aiuo_dict[g].append(alt)

    for g, names in aiuo_dict.items():
        html = f"<h2>{g}ã®ã‚­ãƒã‚³</h2><ul>"
        for n in sorted(names):
            safe = safe_filename(n)
            html += f"<li><a href='{safe}.html'>{n}</a></li>"
        html += "</ul>"
        html += group_links_html
        html += STYLE_TAG + LIGHTGALLERY_TAGS + SCRIPT_TAG

        with open(f"{OUTPUT_DIR}/{safe_filename(g)}.html", "w", encoding="utf-8") as f:
            f.write(html)

    # ---- index ----
    index = "<h2>äº”åéŸ³åˆ¥åˆ†é¡</h2><ul>"
    for g in AIUO_GROUPS.keys():
        index += f"<li><a href='{safe_filename(g)}.html'>{g}</a></li>"
    index += "</ul>"
    index += STYLE_TAG + LIGHTGALLERY_TAGS + SCRIPT_TAG

    with open(f"{OUTPUT_DIR}/index.html", "w", encoding="utf-8") as f:
        f.write(index)

    print("âœ… ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒšãƒ¼ã‚¸ç”Ÿæˆå®Œäº†")

# ==========================
#  ãƒ¡ã‚¤ãƒ³
# ==========================
if __name__ == "__main__":
    fetch_hatena_articles_api()
    entries = fetch_images()
    if entries:
        generate_gallery(entries)
    else:
        print("âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
