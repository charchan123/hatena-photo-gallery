import os, glob, time, requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re

# ====== è¨­å®š ======
HATENA_USER = os.getenv("HATENA_USER")
HATENA_BLOG_ID = os.getenv("HATENA_BLOG_ID")
HATENA_API_KEY = os.getenv("HATENA_API_KEY")

if not all([HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY]):
    raise EnvironmentError("ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™ã€‚HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

ARTICLES_DIR = "articles"
OUTPUT_DIR = "output"

# ====== API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ======
ATOM_ENDPOINT = f"https://blog.hatena.ne.jp/{HATENA_USER}/{HATENA_BLOG_ID}/atom/entry"

AUTH = (HATENA_USER, HATENA_API_KEY)
HEADERS = {}

AIUO_GROUPS = {
    "ã‚è¡Œ": list("ã‚ã„ã†ãˆãŠã‚¢ã‚¤ã‚¦ã‚¨ã‚ª"),
    "ã‹è¡Œ": list("ã‹ããã‘ã“ã‚«ã‚­ã‚¯ã‚±ã‚³ãŒããã’ã”ã‚¬ã‚®ã‚°ã‚²ã‚´"),
    "ã•è¡Œ": list("ã•ã—ã™ã›ãã‚µã‚·ã‚¹ã‚»ã‚½ã–ã˜ãšãœãã‚¶ã‚¸ã‚ºã‚¼ã‚¾"),
    "ãŸè¡Œ": list("ãŸã¡ã¤ã¦ã¨ã‚¿ãƒãƒ„ãƒ†ãƒˆã ã¢ã¥ã§ã©ãƒ€ãƒ‚ãƒ…ãƒ‡ãƒ‰"),
    "ãªè¡Œ": list("ãªã«ã¬ã­ã®ãƒŠãƒ‹ãƒŒãƒãƒ"),
    "ã¯è¡Œ": list("ã¯ã²ãµã¸ã»ãƒãƒ’ãƒ•ãƒ˜ãƒ›ã°ã³ã¶ã¹ã¼ãƒãƒ“ãƒ–ãƒ™ãƒœã±ã´ã·ãºã½ãƒ‘ãƒ”ãƒ—ãƒšãƒ"),
    "ã¾è¡Œ": list("ã¾ã¿ã‚€ã‚ã‚‚ãƒãƒŸãƒ ãƒ¡ãƒ¢"),
    "ã‚„è¡Œ": list("ã‚„ã‚†ã‚ˆãƒ¤ãƒ¦ãƒ¨"),
    "ã‚‰è¡Œ": list("ã‚‰ã‚Šã‚‹ã‚Œã‚ãƒ©ãƒªãƒ«ãƒ¬ãƒ­"),
    "ã‚è¡Œ": list("ã‚ã‚’ã‚“ãƒ¯ãƒ²ãƒ³"),
}

import os, glob, requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re

# ====== è¨­å®š ======
HATENA_USER = os.getenv("HATENA_USER")
HATENA_BLOG_ID = os.getenv("HATENA_BLOG_ID")
HATENA_API_KEY = os.getenv("HATENA_API_KEY")

if not all([HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY]):
    raise EnvironmentError("ç’°å¢ƒå¤‰æ•° HATENA_USER, HATENA_BLOG_ID, HATENA_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

ARTICLES_DIR = "articles"
OUTPUT_DIR = "output"

ATOM_ENDPOINT = f"https://blog.hatena.ne.jp/{HATENA_USER}/{HATENA_BLOG_ID}/atom/entry"
AUTH = (HATENA_USER, HATENA_API_KEY)
HEADERS = {}

AIUO_GROUPS = {
    "ã‚è¡Œ": list("ã‚ã„ã†ãˆãŠã‚¢ã‚¤ã‚¦ã‚¨ã‚ª"),
    "ã‹è¡Œ": list("ã‹ããã‘ã“ã‚«ã‚­ã‚¯ã‚±ã‚³ãŒããã’ã”ã‚¬ã‚®ã‚°ã‚²ã‚´"),
    "ã•è¡Œ": list("ã•ã—ã™ã›ãã‚µã‚·ã‚¹ã‚»ã‚½ã–ã˜ãšãœãã‚¶ã‚¸ã‚ºã‚¼ã‚¾"),
    "ãŸè¡Œ": list("ãŸã¡ã¤ã¦ã¨ã‚¿ãƒãƒ„ãƒ†ãƒˆã ã¢ã¥ã§ã©ãƒ€ãƒ‚ãƒ…ãƒ‡ãƒ‰"),
    "ãªè¡Œ": list("ãªã«ã¬ã­ã®ãƒŠãƒ‹ãƒŒãƒãƒ"),
    "ã¯è¡Œ": list("ã¯ã²ãµã¸ã»ãƒãƒ’ãƒ•ãƒ˜ãƒ›ã°ã³ã¶ã¹ã¼ãƒãƒ“ãƒ–ãƒ™ãƒœã±ã´ã·ãºã½ãƒ‘ãƒ”ãƒ—ãƒšãƒ"),
    "ã¾è¡Œ": list("ã¾ã¿ã‚€ã‚ã‚‚ãƒãƒŸãƒ ãƒ¡ãƒ¢"),
    "ã‚„è¡Œ": list("ã‚„ã‚†ã‚ˆãƒ¤ãƒ¦ãƒ¨"),
    "ã‚‰è¡Œ": list("ã‚‰ã‚Šã‚‹ã‚Œã‚ãƒ©ãƒªãƒ«ãƒ¬ãƒ­"),
    "ã‚è¡Œ": list("ã‚ã‚’ã‚“ãƒ¯ãƒ²ãƒ³"),
}

# ====== iframe é«˜ã•èª¿æ•´ + Masonry + Lightboxç”¨ CSS/JS ======
SCRIPT_STYLE_TAG = """<style>
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; background:#fafafa; color:#333; padding:16px; min-height:0; box-sizing:border-box; }
.gallery-wrapper { width:100%; }
.gallery { position: relative; margin:0 auto; }
.gallery img { display:block; border-radius:8px; transition:opacity 0.5s ease-out; opacity:0; margin-bottom:10px; cursor:pointer; }
.gallery img.visible { opacity:1; }
@media (max-width:400px) { .gallery { width:100% !important; } }
a.back-link { display:inline-block; margin-top:16px; color:#333; text-decoration:none; }
</style>
<script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
<script>
(function() {
  if(window===window.parent) return;
  const sendHeight=()=>{const h=document.documentElement.scrollHeight;window.parent.postMessage({type:"setHeight",height:h},"*");};
  window.addEventListener("load",()=>{sendHeight(); setTimeout(sendHeight,500); setTimeout(sendHeight,1000); window.scrollTo(0,0); setTimeout(()=>window.scrollTo(0,0),200);});
  window.addEventListener("resize",sendHeight);
  const observer=new MutationObserver(sendHeight); observer.observe(document.body,{childList:true,subtree:true});
  setInterval(sendHeight,1000);

  document.addEventListener("DOMContentLoaded",()=>{
    const imgs=document.querySelectorAll(".gallery img");
    const obs=new IntersectionObserver(entries=>{entries.forEach(e=>{if(e.isIntersecting){e.target.classList.add("visible"); obs.unobserve(e.target);}}},{threshold:0.1});
    imgs.forEach(i=>obs.observe(i));

    const gallery=document.querySelector(".gallery");
    if(gallery){
      const gutter=10;
      const columnWidth=160;
      const setMasonryLayout=()=>{
        const isMobile=window.innerWidth<=400;
        const columns=isMobile?1:2;
        const colWidth=isMobile?window.innerWidth-32:columnWidth;
        const galleryWidth=isMobile?window.innerWidth:colWidth*columns+gutter;
        gallery.style.width=galleryWidth+"px"; gallery.style.margin="0 auto";
        gallery.querySelectorAll("img").forEach(img=>img.style.width=colWidth+"px");
        if(gallery.msnry){gallery.msnry.options.columnWidth=colWidth; gallery.msnry.options.fitWidth=false; gallery.msnry.layout();}
        else{gallery.msnry=new Masonry(gallery,{itemSelector:'img',columnWidth:colWidth,gutter:gutter,fitWidth:false});}
      };
      imagesLoaded(gallery,()=>{setMasonryLayout(); sendHeight();});
      window.addEventListener('resize',()=>{setMasonryLayout(); sendHeight();});
    }

    // ===== Lightboxã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼ =====
    const lightbox=document.createElement("div");
    lightbox.id="lightbox"; lightbox.style.display="none"; lightbox.style.position="fixed";
    lightbox.style.top="0"; lightbox.style.left="0"; lightbox.style.width="100%"; lightbox.style.height="100%";
    lightbox.style.background="rgba(0,0,0,0.8)"; lightbox.style.justifyContent="center";
    lightbox.style.alignItems="center"; lightbox.style.zIndex="9999"; lightbox.style.display="flex";
    lightbox.innerHTML=`
      <span id="lightbox-close" style="position:absolute;top:20px;right:30px;font-size:30px;color:white;cursor:pointer;">&times;</span>
      <img id="lightbox-img" src="" style="max-width:90%;max-height:80%;border-radius:8px;">
      <a id="lightbox-prev" style="position:absolute;left:30px;top:50%;transform:translateY(-50%);
         font-size:50px;color:white;text-decoration:none;cursor:pointer;">&#10094;</a>
      <a id="lightbox-next" style="position:absolute;right:30px;top:50%;transform:translateY(-50%);
         font-size:50px;color:white;text-decoration:none;cursor:pointer;">&#10095;</a>
      <a id="lightbox-article" href="#" target="_blank" style="position:absolute;right:30px;bottom:30px;
         color:white;text-decoration:underline;font-size:18px;">ã“ã®è¨˜äº‹ã‚’èª­ã‚€</a>
    `;
    document.body.appendChild(lightbox);

    const lbImg=document.getElementById("lightbox-img");
    const lbClose=document.getElementById("lightbox-close");
    const lbPrev=document.getElementById("lightbox-prev");
    const lbNext=document.getElementById("lightbox-next");
    const lbArticle=document.getElementById("lightbox-article");

    let currentIndex=0;
    const galleryImgs=Array.from(imgs);
    function showLightbox(index){
      currentIndex=index;
      const img=galleryImgs[currentIndex];
      lbImg.src=img.src;
      lbArticle.href=img.dataset.article||"#";
      lightbox.style.display="flex";
    }
    function closeLightbox(){lightbox.style.display="none";}
    function showPrev(){currentIndex=(currentIndex-1+galleryImgs.length)%galleryImgs.length; showLightbox(currentIndex);}
    function showNext(){currentIndex=(currentIndex+1)%galleryImgs.length; showLightbox(currentIndex);}

    imgs.forEach((img,i)=> img.addEventListener("click",()=>showLightbox(i)));
    lbClose.addEventListener("click",closeLightbox);
    lbPrev.addEventListener("click",showPrev);
    lbNext.addEventListener("click",showNext);

    document.addEventListener("keydown",function(e){
      if(e.key==="Escape") closeLightbox();
      if(e.key==="ArrowLeft") showPrev();
      if(e.key==="ArrowRight") showNext();
    });
  });
})();
</script>
"""

# ====== APIã‹ã‚‰å…¨è¨˜äº‹å–å¾— ======
def fetch_hatena_articles_api():
    os.makedirs(ARTICLES_DIR, exist_ok=True)
    print("ğŸ“¡ ã¯ã¦ãªãƒ–ãƒ­ã‚°APIã‹ã‚‰å…¨è¨˜äº‹å–å¾—ä¸­â€¦")
    url = ATOM_ENDPOINT
    count = 0
    while url:
        print(f"ğŸ”— Fetching: {url}")
        r = requests.get(url, auth=AUTH, headers=HEADERS)
        if r.status_code != 200:
            raise RuntimeError(f"âŒ APIå–å¾—å¤±æ•—: {r.status_code} {r.text}")
        root = ET.fromstring(r.text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}
        entries = root.findall("atom:entry", ns)
        for i, entry in enumerate(entries, 1):
            content = entry.find("atom:content", ns)
            if content is None: continue
            html_content = content.text or ""
            article_url = entry.find("atom:link[@rel='alternate']", ns).attrib.get("href", "#")
            filename = f"{ARTICLES_DIR}/article_{count+i}.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(html_content)
            # ä¿å­˜æ™‚ã«è¨˜äº‹URLã‚‚è¿”ã™ãŸã‚ã«ã‚¨ãƒ³ãƒˆãƒªã«è¿½è¨˜
            entry._article_url = article_url
            print(f"âœ… ä¿å­˜å®Œäº†: {filename}")
        count += len(entries)
        next_link = root.find("atom:link[@rel='next']", ns)
        url = next_link.attrib["href"] if next_link is not None else None
    print(f"ğŸ“¦ åˆè¨ˆ {count} ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ====== HTMLã‹ã‚‰ç”»åƒã¨alt + è¨˜äº‹URLã‚’æŠ½å‡º ======
def fetch_images():
    print("ğŸ“‚ HTMLã‹ã‚‰ç”»åƒæŠ½å‡ºä¸­â€¦")
    entries = []
    exclude_patterns = [r'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯', r'^\d{4}å¹´', r'^ã“ã®è¨˜äº‹ã‚’ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ $', r'^ãƒ¯è¡Œ$', r'ã‚­ãƒã‚³ã¨ç”°èˆéŠã³']

    html_files = sorted(glob.glob(f"{ARTICLES_DIR}/*.html"))
    for html_file in html_files:
        with open(html_file, encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            body_div = soup.find(class_="entry-body") or soup
            for iframe in body_div.find_all("iframe"):
                title = iframe.get("title","")
                if any(re.search(p,title) for p in exclude_patterns): iframe.decompose()
            for a in body_div.find_all("a"):
                text = a.get_text(strip=True)
                if any(re.search(p,text) for p in exclude_patterns): a.decompose()
            imgs = body_div.find_all("img")
            for img in imgs:
                alt = img.get("alt","").strip()
                src = img.get("src")
                if not alt or not src: continue
                if any(re.search(p,alt) for p in exclude_patterns): continue
                # è¨˜äº‹URLã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨å®š
                article_url = os.path.basename(html_file)
                entries.append({"alt":alt, "src":src, "article_url":article_url})
    print(f"ğŸ§© ç”»åƒæ¤œå‡ºæ•°: {len(entries)} æš")
    return entries

# ====== äº”åéŸ³åˆ†é¡ ======
def get_aiuo_group(name):
    if not name: return "ãã®ä»–"
    first = name[0]
    for group, chars in AIUO_GROUPS.items():
        if first in chars: return group
    return "ãã®ä»–"

# ====== ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç”Ÿæˆï¼ˆLightboxå¯¾å¿œç‰ˆï¼‰ ======
def generate_gallery(entries):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    grouped = {}
    for e in entries:
        grouped.setdefault(e["alt"], []).append(e)

    group_links = " | ".join([f'<a href="{g}.html">{g}</a>' for g in AIUO_GROUPS.keys()])
    group_links_html = f"<div style='margin-top:40px; text-align:center;'>{group_links}</div>"

    def safe_filename(name):
        return re.sub(r'[:<>\"|*?\\/\r\n]', '_', name).strip() or "unnamed"

    for alt, imgs in grouped.items():
        html = f"<h2>{alt}</h2><div class='gallery'>"
        for img in imgs:
            html += f'<img src="{img["src"]}" alt="{alt}" data-article="{img["article_url"]}" loading="lazy">'
        html += "</div>"
        html += "<div style='margin-top:40px; text-align:center;'><a href='javascript:history.back()' style='text-decoration:none;color:#007acc;'>â† æˆ»ã‚‹</a></div>"
        html += SCRIPT_STYLE_TAG
        safe = safe_filename(alt)
        with open(f"{OUTPUT_DIR}/{safe}.html", "w", encoding="utf-8") as f:
            f.write(html)

    # äº”åéŸ³ãƒšãƒ¼ã‚¸
    aiuo_dict = {k: [] for k in AIUO_GROUPS.keys()}
    for alt in grouped.keys():
        g = get_aiuo_group(alt)
        if g in aiuo_dict: aiuo_dict[g].append(alt)

    for g, names in aiuo_dict.items():
        html = f"<h2>{g}ã®ã‚­ãƒã‚³</h2><ul>"
        for n in sorted(names):
            safe = safe_filename(n)
            html += f'<li><a href="{safe}.html">{n}</a></li>'
        html += "</ul>" + group_links_html + SCRIPT_STYLE_TAG
        with open(f"{OUTPUT_DIR}/{safe_filename(g)}.html", "w", encoding="utf-8") as f:
            f.write(html)

    index = "<h2>äº”åéŸ³åˆ¥åˆ†é¡</h2><ul>"
    for g in AIUO_GROUPS.keys():
        index += f'<li><a href="{safe_filename(g)}.html">{g}</a></li>'
    index += "</ul>" + SCRIPT_STYLE_TAG
    with open(f"{OUTPUT_DIR}/index.html", "w", encoding="utf-8") as f:
        f.write(index)

    print("âœ… ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒšãƒ¼ã‚¸ç”Ÿæˆå®Œäº†")

# ====== ãƒ¡ã‚¤ãƒ³ ======
if __name__ == "__main__":
    fetch_hatena_articles_api()
    entries = fetch_images()
    if entries:
        generate_gallery(entries)
    else:
        print("âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ====== APIã‹ã‚‰å…¨è¨˜äº‹ã‚’å–å¾— ======
def fetch_hatena_articles_api():
    os.makedirs(ARTICLES_DIR, exist_ok=True)
    print(f"ğŸ“¡ ã¯ã¦ãªãƒ–ãƒ­ã‚°APIã‹ã‚‰å…¨è¨˜äº‹å–å¾—ä¸­â€¦")

    url = ATOM_ENDPOINT
    count = 0
    while url:
        print(f"ğŸ”— Fetching: {url}")
        r = requests.get(url, auth=AUTH, headers=HEADERS)
        if r.status_code != 200:
            raise RuntimeError(f"âŒ APIå–å¾—å¤±æ•—: {r.status_code} {r.text}")
        root = ET.fromstring(r.text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}
        entries = root.findall("atom:entry", ns)
        for i, entry in enumerate(entries, 1):
            content = entry.find("atom:content", ns)
            if content is None: continue
            html_content = content.text or ""
            filename = f"{ARTICLES_DIR}/article_{count+i}.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"âœ… ä¿å­˜å®Œäº†: {filename}")
        count += len(entries)
        next_link = root.find("atom:link[@rel='next']", ns)
        url = next_link.attrib["href"] if next_link is not None else None

    print(f"ğŸ“¦ åˆè¨ˆ {count} ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ====== HTMLã‹ã‚‰ç”»åƒã¨altã‚’æŠ½å‡ºï¼ˆæœ¬æ–‡é™å®š + altãƒ•ã‚£ãƒ«ã‚¿ + iframe/aé™¤å¤–ï¼‰ ======
def fetch_images():
    import re

    print("ğŸ“‚ HTMLã‹ã‚‰ç”»åƒæŠ½å‡ºä¸­â€¦")
    entries = []

    # é™¤å¤–ã—ãŸã„ alt/text ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
    exclude_patterns = [
        r'ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',                  # éƒ¨åˆ†ä¸€è‡´
        r'^\d{4}å¹´',                             # å¹´ä»˜ããƒ†ã‚­ã‚¹ãƒˆ
        r'^ã“ã®è¨˜äº‹ã‚’ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã«è¿½åŠ $', # å®Œå…¨ä¸€è‡´
        r'^ãƒ¯è¡Œ$',                               # å®Œå…¨ä¸€è‡´
        r'ã‚­ãƒã‚³ã¨ç”°èˆéŠã³',  # éƒ¨åˆ†ä¸€è‡´
        # è¿½åŠ ã™ã‚‹å ´åˆã¯ã“ã“ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½è¨˜
    ]

    for html_file in glob.glob(f"{ARTICLES_DIR}/*.html"):
        with open(html_file, encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            # æœ¬æ–‡ã«é™å®šï¼ˆentry-body ã‚¯ãƒ©ã‚¹å†…ï¼‰
            body_div = soup.find(class_="entry-body")
            if not body_div:
                body_div = soup  # æœ¬æ–‡é™å®šãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ä½“

            # ===== iframe / a ã‚¿ã‚°ã§é™¤å¤–å¯¾è±¡ã‚’å‰Šé™¤ =====
            for iframe in body_div.find_all("iframe"):
                title = iframe.get("title", "")
                if any(re.search(p, title) for p in exclude_patterns):
                    iframe.decompose()
            for a in body_div.find_all("a"):
                text = a.get_text(strip=True)
                if any(re.search(p, text) for p in exclude_patterns):
                    a.decompose()

            # ===== img ã‚¿ã‚°ã‚’æŠ½å‡º =====
            imgs = body_div.find_all("img")
            for img in imgs:
                alt = img.get("alt", "").strip()
                src = img.get("src")
                if not alt or not src:
                    continue
                if any(re.search(p, alt) for p in exclude_patterns):
                    continue
                entries.append({"alt": alt, "src": src})

    print(f"ğŸ§© ç”»åƒæ¤œå‡ºæ•°: {len(entries)} æš")
    return entries

# ====== äº”åéŸ³åˆ†é¡ ======
def get_aiuo_group(name):
    if not name:
        return "ãã®ä»–"
    first = name[0]
    for group, chars in AIUO_GROUPS.items():
        if first in chars:
            return group
    return "ãã®ä»–"

# ====== ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç”Ÿæˆ ======
def generate_gallery(entries):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    grouped = {}
    for e in entries:
        grouped.setdefault(e["alt"], []).append(e["src"])

    # å…±é€šãƒªãƒ³ã‚¯
    group_links = " | ".join([f'<a href="{g}.html">{g}</a>' for g in AIUO_GROUPS.keys()])
    group_links_html = f"<div style='margin-top:40px; text-align:center;'>{group_links}</div>"

    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆé–¢æ•°
    def safe_filename(name):
        import re
        name = re.sub(r'[:<>\"|*?\\/\r\n]', '_', name)  # ç¦æ­¢æ–‡å­—ã‚’ _
        name = name.strip()
        if not name:
            name = "unnamed"
        return name

    # å„ã‚­ãƒã‚³ãƒšãƒ¼ã‚¸
    for alt, imgs in grouped.items():
        html = f"<h2>{alt}</h2><div class='gallery'>"
        for src in imgs:
            html += f'<img src="{src}" alt="{alt}" loading="lazy">'
        html += "</div>"
        html += """
        <div style='margin-top:40px; text-align:center;'>
          <a href='javascript:history.back()' style='text-decoration:none;color:#007acc;'>â† æˆ»ã‚‹</a>
        </div>
        """
        html += SCRIPT_STYLE_TAG
        safe = safe_filename(alt)
        with open(f"{OUTPUT_DIR}/{safe}.html", "w", encoding="utf-8") as f:
            f.write(html)

    # äº”åéŸ³ãƒšãƒ¼ã‚¸
    aiuo_dict = {k: [] for k in AIUO_GROUPS.keys()}
    for alt in grouped.keys():
        g = get_aiuo_group(alt)
        if g in aiuo_dict:
            aiuo_dict[g].append(alt)

    for g, names in aiuo_dict.items():
        html = f"<h2>{g}ã®ã‚­ãƒã‚³</h2><ul>"
        for n in sorted(names):
            safe = safe_filename(n)
            html += f'<li><a href="{safe}.html">{n}</a></li>'
        html += "</ul>"
        html += group_links_html
        html += SCRIPT_STYLE_TAG
        with open(f"{OUTPUT_DIR}/{safe_filename(g)}.html", "w", encoding="utf-8") as f:
            f.write(html)

    # index.html
    index = "<h2>äº”åéŸ³åˆ¥åˆ†é¡</h2><ul>"
    for g in AIUO_GROUPS.keys():
        index += f'<li><a href="{safe_filename(g)}.html">{g}</a></li>'
    index += "</ul>" + SCRIPT_STYLE_TAG
    with open(f"{OUTPUT_DIR}/index.html", "w", encoding="utf-8") as f:
        f.write(index)

    print("âœ… ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒšãƒ¼ã‚¸ç”Ÿæˆå®Œäº†")

# ====== ãƒ¡ã‚¤ãƒ³ ======
if __name__ == "__main__":
    fetch_hatena_articles_api()
    entries = fetch_images()
    if entries:
        generate_gallery(entries)
    else:
        print("âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
